const fs = require('fs').promises;\nconst path = require('path');\n\n/**\n * UserAcceptanceTesting - A/B testing and user feedback collection framework\n * Provides tools for quality comparison, user feedback collection, and satisfaction metrics\n */\nclass UserAcceptanceTesting {\n  constructor() {\n    this.testSuites = new Map();\n    this.userSessions = new Map();\n    this.feedbackData = [];\n    this.comparisonTests = [];\n    \n    this.testResultsFile = path.join(__dirname, '../../data/user_acceptance_tests.json');\n    this.feedbackFile = path.join(__dirname, '../../data/user_feedback.json');\n    \n    // Load existing data\n    this.loadTestData();\n  }\n\n  /**\n   * Create a new A/B test for generation quality comparison\n   * @param {Object} testConfig - Test configuration\n   * @returns {string} Test ID\n   */\n  async createABTest(testConfig) {\n    const {\n      name,\n      description,\n      variants,\n      targetUsers,\n      duration,\n      successMetrics,\n      testType = 'quality_comparison'\n    } = testConfig;\n    \n    const testId = `test_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    const test = {\n      id: testId,\n      name,\n      description,\n      type: testType,\n      status: 'active',\n      createdAt: Date.now(),\n      startDate: Date.now(),\n      endDate: Date.now() + (duration * 24 * 60 * 60 * 1000), // duration in days\n      variants: variants.map((variant, index) => ({\n        id: `variant_${index}`,\n        name: variant.name,\n        description: variant.description,\n        parameters: variant.parameters,\n        weight: variant.weight || 0.5, // Traffic allocation\n        results: {\n          impressions: 0,\n          conversions: 0,\n          ratings: [],\n          feedback: []\n        }\n      })),\n      targetUsers: targetUsers || 'all',\n      successMetrics: successMetrics || {\n        primaryMetric: 'user_rating',\n        threshold: 4.0,\n        minSampleSize: 50\n      },\n      metadata: {\n        creator: 'system',\n        tags: testConfig.tags || []\n      }\n    };\n    \n    this.testSuites.set(testId, test);\n    await this.saveTestData();\n    \n    console.log(`âœ… Created A/B test: ${name} (${testId})`);\n    return testId;\n  }\n\n  /**\n   * Get variant assignment for a user session\n   * @param {string} testId - Test identifier\n   * @param {string} userId - User identifier\n   * @param {Object} context - Request context\n   * @returns {Object} Variant assignment\n   */\n  getVariantAssignment(testId, userId, context = {}) {\n    const test = this.testSuites.get(testId);\n    \n    if (!test || test.status !== 'active' || Date.now() > test.endDate) {\n      return null;\n    }\n    \n    // Check if user already has an assignment\n    const sessionKey = `${testId}_${userId}`;\n    if (this.userSessions.has(sessionKey)) {\n      return this.userSessions.get(sessionKey);\n    }\n    \n    // Assign variant based on weights\n    const random = Math.random();\n    let cumulativeWeight = 0;\n    \n    for (const variant of test.variants) {\n      cumulativeWeight += variant.weight;\n      if (random <= cumulativeWeight) {\n        const assignment = {\n          testId,\n          variantId: variant.id,\n          variant: variant,\n          assignedAt: Date.now(),\n          userId,\n          context\n        };\n        \n        this.userSessions.set(sessionKey, assignment);\n        \n        // Increment impressions\n        variant.results.impressions++;\n        \n        return assignment;\n      }\n    }\n    \n    // Fallback to first variant\n    const fallbackVariant = test.variants[0];\n    const assignment = {\n      testId,\n      variantId: fallbackVariant.id,\n      variant: fallbackVariant,\n      assignedAt: Date.now(),\n      userId,\n      context\n    };\n    \n    this.userSessions.set(sessionKey, assignment);\n    fallbackVariant.results.impressions++;\n    \n    return assignment;\n  }\n\n  /**\n   * Record user interaction with test variant\n   * @param {string} testId - Test identifier\n   * @param {string} userId - User identifier\n   * @param {Object} interaction - Interaction data\n   */\n  async recordInteraction(testId, userId, interaction) {\n    const test = this.testSuites.get(testId);\n    const sessionKey = `${testId}_${userId}`;\n    const session = this.userSessions.get(sessionKey);\n    \n    if (!test || !session) {\n      console.warn(`No active test session found for ${testId}/${userId}`);\n      return;\n    }\n    \n    const variant = test.variants.find(v => v.id === session.variantId);\n    if (!variant) {\n      console.warn(`Variant ${session.variantId} not found in test ${testId}`);\n      return;\n    }\n    \n    const {\n      type,\n      rating,\n      feedback,\n      generationId,\n      qualityScore,\n      conversionEvent,\n      metadata\n    } = interaction;\n    \n    // Record the interaction\n    const interactionRecord = {\n      timestamp: Date.now(),\n      type,\n      userId,\n      generationId,\n      rating,\n      feedback,\n      qualityScore,\n      metadata: metadata || {}\n    };\n    \n    // Update variant results\n    if (rating) {\n      variant.results.ratings.push(rating);\n    }\n    \n    if (feedback) {\n      variant.results.feedback.push({\n        timestamp: Date.now(),\n        userId,\n        feedback,\n        generationId\n      });\n    }\n    \n    if (conversionEvent) {\n      variant.results.conversions++;\n    }\n    \n    // Store in global feedback data\n    this.feedbackData.push({\n      ...interactionRecord,\n      testId,\n      variantId: session.variantId\n    });\n    \n    await this.saveTestData();\n    await this.saveFeedbackData();\n    \n    console.log(`ðŸ“Š Recorded interaction for test ${testId}, variant ${session.variantId}`);\n  }\n\n  /**\n   * Create a quality comparison test between two generation approaches\n   * @param {Object} comparisonConfig - Comparison configuration\n   * @returns {string} Comparison test ID\n   */\n  async createQualityComparison(comparisonConfig) {\n    const {\n      name,\n      description,\n      baselineParameters,\n      testParameters,\n      testCases,\n      evaluationCriteria\n    } = comparisonConfig;\n    \n    const comparisonId = `comparison_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    const comparison = {\n      id: comparisonId,\n      name,\n      description,\n      status: 'active',\n      createdAt: Date.now(),\n      baseline: {\n        name: 'Baseline',\n        parameters: baselineParameters,\n        results: []\n      },\n      test: {\n        name: 'Test Variant',\n        parameters: testParameters,\n        results: []\n      },\n      testCases: testCases.map((testCase, index) => ({\n        id: `case_${index}`,\n        ...testCase,\n        status: 'pending',\n        results: {\n          baseline: null,\n          test: null,\n          comparison: null\n        }\n      })),\n      evaluationCriteria: evaluationCriteria || {\n        consistency: { weight: 0.4, threshold: 0.7 },\n        accuracy: { weight: 0.4, threshold: 0.7 },\n        userPreference: { weight: 0.2, threshold: 0.6 }\n      },\n      overallResults: {\n        winner: null,\n        confidence: 0,\n        statisticalSignificance: false,\n        summary: null\n      }\n    };\n    \n    this.comparisonTests.push(comparison);\n    await this.saveTestData();\n    \n    console.log(`ðŸ”¬ Created quality comparison: ${name} (${comparisonId})`);\n    return comparisonId;\n  }\n\n  /**\n   * Execute a test case in a quality comparison\n   * @param {string} comparisonId - Comparison test ID\n   * @param {string} testCaseId - Test case ID\n   * @param {Function} generationFunction - Function to generate images\n   * @returns {Object} Test case results\n   */\n  async executeTestCase(comparisonId, testCaseId, generationFunction) {\n    const comparison = this.comparisonTests.find(c => c.id === comparisonId);\n    if (!comparison) {\n      throw new Error(`Comparison test ${comparisonId} not found`);\n    }\n    \n    const testCase = comparison.testCases.find(tc => tc.id === testCaseId);\n    if (!testCase) {\n      throw new Error(`Test case ${testCaseId} not found`);\n    }\n    \n    console.log(`ðŸ§ª Executing test case: ${testCase.name}`);\n    \n    try {\n      testCase.status = 'running';\n      \n      // Generate baseline result\n      console.log('Generating baseline result...');\n      const baselineResult = await generationFunction({\n        ...testCase.input,\n        parameters: comparison.baseline.parameters\n      });\n      \n      // Generate test result\n      console.log('Generating test result...');\n      const testResult = await generationFunction({\n        ...testCase.input,\n        parameters: comparison.test.parameters\n      });\n      \n      // Store results\n      testCase.results.baseline = {\n        ...baselineResult,\n        timestamp: Date.now()\n      };\n      \n      testCase.results.test = {\n        ...testResult,\n        timestamp: Date.now()\n      };\n      \n      // Perform automated comparison\n      const comparisonResult = this.compareResults(\n        baselineResult,\n        testResult,\n        comparison.evaluationCriteria\n      );\n      \n      testCase.results.comparison = comparisonResult;\n      testCase.status = 'completed';\n      \n      // Update overall comparison results\n      comparison.baseline.results.push(baselineResult);\n      comparison.test.results.push(testResult);\n      \n      await this.saveTestData();\n      \n      console.log(`âœ… Test case completed: ${testCase.name}`);\n      return testCase.results;\n      \n    } catch (error) {\n      testCase.status = 'failed';\n      testCase.error = error.message;\n      \n      console.error(`âŒ Test case failed: ${testCase.name}`, error);\n      throw error;\n    }\n  }\n\n  /**\n   * Compare two generation results\n   * @param {Object} baseline - Baseline result\n   * @param {Object} test - Test result\n   * @param {Object} criteria - Evaluation criteria\n   * @returns {Object} Comparison result\n   */\n  compareResults(baseline, test, criteria) {\n    const comparison = {\n      timestamp: Date.now(),\n      scores: {},\n      winner: null,\n      confidence: 0,\n      details: {}\n    };\n    \n    // Compare consistency scores\n    if (baseline.validation && test.validation) {\n      const baselineConsistency = baseline.validation.consistency?.overallScore || 0;\n      const testConsistency = test.validation.consistency?.overallScore || 0;\n      \n      comparison.scores.consistency = {\n        baseline: baselineConsistency,\n        test: testConsistency,\n        difference: testConsistency - baselineConsistency,\n        winner: testConsistency > baselineConsistency ? 'test' : 'baseline'\n      };\n    }\n    \n    // Compare accuracy scores\n    if (baseline.validation && test.validation) {\n      const baselineAccuracy = baseline.validation.accuracy?.overallScore || 0;\n      const testAccuracy = test.validation.accuracy?.overallScore || 0;\n      \n      comparison.scores.accuracy = {\n        baseline: baselineAccuracy,\n        test: testAccuracy,\n        difference: testAccuracy - baselineAccuracy,\n        winner: testAccuracy > baselineAccuracy ? 'test' : 'baseline'\n      };\n    }\n    \n    // Compare overall quality\n    const baselineQuality = baseline.validation?.overallQuality || 0;\n    const testQuality = test.validation?.overallQuality || 0;\n    \n    comparison.scores.overall = {\n      baseline: baselineQuality,\n      test: testQuality,\n      difference: testQuality - baselineQuality,\n      winner: testQuality > baselineQuality ? 'test' : 'baseline'\n    };\n    \n    // Calculate weighted score\n    let weightedScore = 0;\n    let totalWeight = 0;\n    \n    Object.entries(criteria).forEach(([metric, config]) => {\n      if (comparison.scores[metric]) {\n        const score = comparison.scores[metric];\n        const normalizedDiff = Math.max(-1, Math.min(1, score.difference));\n        weightedScore += normalizedDiff * config.weight;\n        totalWeight += config.weight;\n      }\n    });\n    \n    if (totalWeight > 0) {\n      const finalScore = weightedScore / totalWeight;\n      comparison.winner = finalScore > 0 ? 'test' : 'baseline';\n      comparison.confidence = Math.abs(finalScore);\n    }\n    \n    return comparison;\n  }\n\n  /**\n   * Analyze A/B test results\n   * @param {string} testId - Test identifier\n   * @returns {Object} Test analysis\n   */\n  analyzeABTest(testId) {\n    const test = this.testSuites.get(testId);\n    if (!test) {\n      throw new Error(`Test ${testId} not found`);\n    }\n    \n    const analysis = {\n      testId,\n      testName: test.name,\n      status: test.status,\n      duration: Date.now() - test.startDate,\n      variants: [],\n      winner: null,\n      statisticalSignificance: false,\n      confidence: 0,\n      recommendations: []\n    };\n    \n    // Analyze each variant\n    test.variants.forEach(variant => {\n      const ratings = variant.results.ratings;\n      const avgRating = ratings.length > 0 ? \n        ratings.reduce((sum, r) => sum + r, 0) / ratings.length : 0;\n      \n      const conversionRate = variant.results.impressions > 0 ? \n        variant.results.conversions / variant.results.impressions : 0;\n      \n      const variantAnalysis = {\n        id: variant.id,\n        name: variant.name,\n        impressions: variant.results.impressions,\n        conversions: variant.results.conversions,\n        conversionRate,\n        averageRating: avgRating,\n        ratingCount: ratings.length,\n        feedbackCount: variant.results.feedback.length,\n        sampleSize: Math.max(ratings.length, variant.results.conversions)\n      };\n      \n      analysis.variants.push(variantAnalysis);\n    });\n    \n    // Determine winner\n    if (analysis.variants.length >= 2) {\n      const primaryMetric = test.successMetrics.primaryMetric;\n      \n      let bestVariant = analysis.variants[0];\n      analysis.variants.forEach(variant => {\n        if (primaryMetric === 'user_rating' && variant.averageRating > bestVariant.averageRating) {\n          bestVariant = variant;\n        } else if (primaryMetric === 'conversion_rate' && variant.conversionRate > bestVariant.conversionRate) {\n          bestVariant = variant;\n        }\n      });\n      \n      analysis.winner = bestVariant;\n      \n      // Calculate statistical significance (simplified)\n      const minSampleSize = test.successMetrics.minSampleSize || 30;\n      const hasSignificantSample = bestVariant.sampleSize >= minSampleSize;\n      \n      if (hasSignificantSample) {\n        const otherVariants = analysis.variants.filter(v => v.id !== bestVariant.id);\n        const avgOtherPerformance = otherVariants.reduce((sum, v) => {\n          return sum + (primaryMetric === 'user_rating' ? v.averageRating : v.conversionRate);\n        }, 0) / otherVariants.length;\n        \n        const winnerPerformance = primaryMetric === 'user_rating' ? \n          bestVariant.averageRating : bestVariant.conversionRate;\n        \n        const improvement = (winnerPerformance - avgOtherPerformance) / avgOtherPerformance;\n        analysis.confidence = Math.min(0.99, Math.abs(improvement));\n        analysis.statisticalSignificance = improvement > 0.05; // 5% improvement threshold\n      }\n    }\n    \n    // Generate recommendations\n    if (analysis.winner && analysis.statisticalSignificance) {\n      analysis.recommendations.push(\n        `Implement ${analysis.winner.name} variant - shows ${(analysis.confidence * 100).toFixed(1)}% improvement`\n      );\n    } else if (analysis.variants.some(v => v.sampleSize < test.successMetrics.minSampleSize)) {\n      analysis.recommendations.push(\n        'Continue test - insufficient sample size for statistical significance'\n      );\n    } else {\n      analysis.recommendations.push(\n        'No clear winner detected - consider testing different variants or parameters'\n      );\n    }\n    \n    return analysis;\n  }\n\n  /**\n   * Get user satisfaction metrics\n   * @param {Object} filters - Filter criteria\n   * @returns {Object} Satisfaction metrics\n   */\n  getUserSatisfactionMetrics(filters = {}) {\n    const {\n      startDate,\n      endDate,\n      modelId,\n      qualityTier,\n      testId\n    } = filters;\n    \n    let filteredFeedback = this.feedbackData;\n    \n    // Apply filters\n    if (startDate) {\n      filteredFeedback = filteredFeedback.filter(f => f.timestamp >= new Date(startDate).getTime());\n    }\n    \n    if (endDate) {\n      filteredFeedback = filteredFeedback.filter(f => f.timestamp <= new Date(endDate).getTime());\n    }\n    \n    if (testId) {\n      filteredFeedback = filteredFeedback.filter(f => f.testId === testId);\n    }\n    \n    if (filteredFeedback.length === 0) {\n      return {\n        totalResponses: 0,\n        averageRating: 0,\n        satisfactionRate: 0,\n        npsScore: 0,\n        breakdown: {},\n        trends: []\n      };\n    }\n    \n    // Calculate metrics\n    const ratings = filteredFeedback.filter(f => f.rating).map(f => f.rating);\n    const averageRating = ratings.length > 0 ? \n      ratings.reduce((sum, r) => sum + r, 0) / ratings.length : 0;\n    \n    const satisfiedUsers = ratings.filter(r => r >= 4).length;\n    const satisfactionRate = ratings.length > 0 ? satisfiedUsers / ratings.length : 0;\n    \n    // Calculate NPS (Net Promoter Score) - simplified\n    const promoters = ratings.filter(r => r >= 4).length;\n    const detractors = ratings.filter(r => r <= 2).length;\n    const npsScore = ratings.length > 0 ? \n      ((promoters - detractors) / ratings.length) * 100 : 0;\n    \n    // Rating breakdown\n    const breakdown = {};\n    for (let i = 1; i <= 5; i++) {\n      breakdown[i] = ratings.filter(r => r === i).length;\n    }\n    \n    return {\n      totalResponses: filteredFeedback.length,\n      averageRating,\n      satisfactionRate,\n      npsScore,\n      breakdown,\n      recentFeedback: filteredFeedback.slice(-20)\n    };\n  }\n\n  /**\n   * Generate comprehensive test report\n   * @param {string} testId - Test identifier\n   * @returns {Object} Comprehensive test report\n   */\n  generateTestReport(testId) {\n    const test = this.testSuites.get(testId);\n    if (!test) {\n      throw new Error(`Test ${testId} not found`);\n    }\n    \n    const analysis = this.analyzeABTest(testId);\n    const satisfactionMetrics = this.getUserSatisfactionMetrics({ testId });\n    \n    return {\n      test: {\n        id: test.id,\n        name: test.name,\n        description: test.description,\n        status: test.status,\n        duration: Date.now() - test.startDate,\n        startDate: new Date(test.startDate).toISOString(),\n        endDate: new Date(test.endDate).toISOString()\n      },\n      analysis,\n      satisfaction: satisfactionMetrics,\n      recommendations: [\n        ...analysis.recommendations,\n        ...(satisfactionMetrics.averageRating < 3.5 ? \n          ['Low user satisfaction detected - review generation quality'] : []),\n        ...(satisfactionMetrics.npsScore < 0 ? \n          ['Negative NPS score - investigate user experience issues'] : [])\n      ],\n      exportData: {\n        variants: test.variants,\n        userSessions: Array.from(this.userSessions.values()).filter(s => s.testId === testId),\n        feedback: this.feedbackData.filter(f => f.testId === testId)\n      }\n    };\n  }\n\n  /**\n   * Load test data from files\n   */\n  async loadTestData() {\n    try {\n      const testData = await fs.readFile(this.testResultsFile, 'utf8');\n      const data = JSON.parse(testData);\n      \n      if (data.testSuites) {\n        this.testSuites = new Map(data.testSuites);\n      }\n      \n      if (data.userSessions) {\n        this.userSessions = new Map(data.userSessions);\n      }\n      \n      if (data.comparisonTests) {\n        this.comparisonTests = data.comparisonTests;\n      }\n    } catch (error) {\n      console.log('Starting with empty test data (file not found or invalid)');\n    }\n  }\n\n  /**\n   * Save test data to files\n   */\n  async saveTestData() {\n    try {\n      const dir = path.dirname(this.testResultsFile);\n      await fs.mkdir(dir, { recursive: true });\n      \n      const data = {\n        testSuites: Array.from(this.testSuites.entries()),\n        userSessions: Array.from(this.userSessions.entries()),\n        comparisonTests: this.comparisonTests,\n        lastUpdated: Date.now()\n      };\n      \n      await fs.writeFile(this.testResultsFile, JSON.stringify(data, null, 2));\n    } catch (error) {\n      console.error('Failed to save test data:', error);\n    }\n  }\n\n  /**\n   * Load feedback data from file\n   */\n  async loadFeedbackData() {\n    try {\n      const feedbackData = await fs.readFile(this.feedbackFile, 'utf8');\n      this.feedbackData = JSON.parse(feedbackData);\n    } catch (error) {\n      console.log('Starting with empty feedback data (file not found or invalid)');\n    }\n  }\n\n  /**\n   * Save feedback data to file\n   */\n  async saveFeedbackData() {\n    try {\n      const dir = path.dirname(this.feedbackFile);\n      await fs.mkdir(dir, { recursive: true });\n      \n      // Keep only last 10000 feedback entries\n      const feedbackToSave = this.feedbackData.slice(-10000);\n      \n      await fs.writeFile(this.feedbackFile, JSON.stringify(feedbackToSave, null, 2));\n    } catch (error) {\n      console.error('Failed to save feedback data:', error);\n    }\n  }\n\n  /**\n   * Get testing statistics\n   * @returns {Object} Testing statistics\n   */\n  getStatistics() {\n    return {\n      activeTests: Array.from(this.testSuites.values()).filter(t => t.status === 'active').length,\n      totalTests: this.testSuites.size,\n      totalSessions: this.userSessions.size,\n      totalFeedback: this.feedbackData.length,\n      comparisonTests: this.comparisonTests.length,\n      version: '1.0.0'\n    };\n  }\n}\n\nmodule.exports = UserAcceptanceTesting;